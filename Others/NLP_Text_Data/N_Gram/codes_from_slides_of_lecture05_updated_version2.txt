import numpy
import scipy
def build_ngram(textindexvector,n_vocab,maxN):
    # Create the overall structure that will store the n-gram
    allprobstructs_NextCount=[]
    allprobstructs_NextProb=[]
    allprobstructs_PreviousStruct=[]

    # Create unigram probability table, store it as the root of probtables
    tempstruct_NextCount=scipy.sparse.dok_matrix((n_vocab,1))
    tempstruct_NextProb=scipy.sparse.dok_matrix((n_vocab,1))
    tempstruct_PreviousStruct=scipy.sparse.dok_matrix((n_vocab,1))

    allprobstructs_NextCount.append(tempstruct_NextCount)
    allprobstructs_NextProb.append(tempstruct_NextProb)
    allprobstructs_PreviousStruct.append(tempstruct_PreviousStruct)
    nstructs=1

    # Count how many probability tables have been created at different
    # n-gram levels. Because this is a zero-based index, the index of the
    # level indicating how long a history each n-gram takes into account:
    # 0 for unigrams, 1 for bigrams, and so on.
    nstructsperlevel=numpy.zeros((maxN))
    # Initially there is only one table which is a unigram-table.
    nstructsperlevel[0]=1



    %--------------------



    # Iterate through the text
    for t in range(len(textindexvector)):
        if (t%10)==0:
            print(str(t) + ' ' + str(nstructsperlevel))
        # Vocabulary index of the word at position t in the text
        tempword=textindexvector[t-0];

        # Suppose we have words w(1),...,w(t-3),w(t-2),w(t-1),w(t) in the text.
        # Then the transition to w(t) must be recorded for several n-grams:
        # []-->w(t) : unigram transition (n=1)
        # [w(t-1)]-->w(t) : bigram transition (n=2)
        # [w(t-2),w(t-1)]-->w(t) : trigram transition (n=3)
        # [w(t-3),w(t-2),w(t-1)]-->w(t) : 4-gram transition (n=4)
        # [w(t-4),w(t-3),w(t-2),w(t-1)]-->w(t) : 5-gram transition (n=5)
     
        # Start from the unigram (root of the tables), record the transition
        currentstruct=0
        # Record the transition into the transition counts:
        # and its count in the unigram model increases by 1.
        #allprobstructs[currentstruct]['Next'][tempword,0]=1
        allprobstructs_NextCount[currentstruct][tempword,0]+=1
      
        # Now record this transition into higher-level n-grams.      
        # Address history up to maximum N-gram length or beginning of 
        # the text, whichever is sooner.
        # Iterate a zero-based index "n" of steps back
        for n in range(min([maxN-1,t])):   
            # Take the next step back.
            # Vocabulary index of the previous word
            previousword=textindexvector[t-n-1];



            %--------------------



            # At this point in the for loop, the current probability table 
            # allprobstructs[currentstruct] represents a (n+1)-gram which uses 
            # a history of length (n): "[w(t-n),...,w(t-1)]--->NextWord". 
            # The "previousword" w(t-n-1) is an expansion of it into a  
            # (n+2)-gram which uses a history of length (n+1): 
            # "[w(t-n-1),...,w(t-1)]--->NextWord". This expansion might exist 
            # already or not. The field 'Previous' of the current (n+1)-gram 
            # records whether that expansion exists.

            # Create a new history reference (next-level ngram) if it 
            # did not exist.
            # Note that the unigram table has index 0, but it is never an 
            # expansion of a smaller n-gram.
            if allprobstructs_PreviousStruct[currentstruct][previousword,0]==0:
                # Create the probability table for the expansion. Because this
                # is the first time this "[w(t-n-1),...,w(t-1)]--->NextWord" has
                # been needed, initially it has no next-words (the current
                # word will become its first observed next-word). Similarly,
                # because this is the first time this table is needed, it cannot
                # have any higher-level expansions yet.



                %--------------------



                tempstruct_NextCount=scipy.sparse.dok_matrix((n_vocab,1));
                tempstruct_NextProb=scipy.sparse.dok_matrix((n_vocab,1));
                tempstruct_PreviousStruct=scipy.sparse.dok_matrix((n_vocab,1));
                # Add the created table into the overall list of all probability 
                # tables, increase the count of tables overall and at the n-gram
                # level (history length n+1) where the table was created.
                nstructs+=1;
                nstructsperlevel[n+1]+=1;
                allprobstructs_NextCount.append(tempstruct_NextCount)
                allprobstructs_NextProb.append(tempstruct_NextProb)
                allprobstructs_PreviousStruct.append(tempstruct_PreviousStruct)
                # Mark that the expansion now exists into the current
                # current "[w(t-n),...,w(t-1)]--->NextWord" table
                # allprobstructs[currentstruct]['Previous'][previousword,0]=1
                # Add a pointer from the current table to the newly 
                # created structure (index of the newly created table in the 
                # overall list)
                allprobstructs_PreviousStruct \
                    [currentstruct][previousword,0]=nstructs-1;
            # At this point we can be sure the next-level n-gram exists, so we
            # go to the next-level ngram and add the newest word-occurrence to it
            # as a possible next word, increasing its count.
            currentstruct=allprobstructs_PreviousStruct \
                [currentstruct][previousword,0];
            currentstruct=int(currentstruct)
            allprobstructs_NextCount[currentstruct][tempword,0]+=1
    # For all tables that have been created, obtain their probabilities by
    # normalizing their counts
    for k in range(nstructs):
        allprobstructs_NextProb[k]=allprobstructs_NextCount[k] \ 
            /numpy.sum(allprobstructs_NextCount[k]);
    return((allprobstructs_NextCount,allprobstructs_NextProb,allprobstructs_PreviousStruct)) 



%--------------------



import numpy
import scipy
import scipy.stats
def sample_ngram(allprobstructs,n_words_to_sample,maxN,initialtext):
    allprobstructs_NextProb=allprobstructs[1]
    allprobstructs_PreviousStruct=allprobstructs[2]
    sampletext=[]
    sampletext.extend(initialtext)
    for k in range(n_words_to_sample):
        # We are sampling a new word for position t
        t=len(initialtext)+k
        # Start from unigram probability table 
        currentstruct=0
        # Try to use as much history as possible for sampling the next
        # word, but revert to smaller n-gram if data is not available for
        # the current history
        historycount=0        
        for n in range(min([maxN-1,t])):
            # If we want, we can set a probability to use a higher-level n-gram
            usehigherlevel_probability=0.99
            if (scipy.stats.uniform.rvs() < usehigherlevel_probability):
                # Try to advance to the next-level n-gram
                historycount=historycount+1
                #print((t,historycount,len(sampletext)))
                previousword=sampletext[t-historycount]
                if allprobstructs_PreviousStruct[currentstruct][previousword,0]>0:
                    currentstruct=allprobstructs_PreviousStruct[currentstruct][previousword,0]
                    currentstruct=int(currentstruct)
                else:
                    # Don't try to advance any more times, just exist the for-loop
                    break
        # At this point we have found a probability table at some history level.
        # Sample from its nonzero entries.
        possiblewords=allprobstructs_NextProb[currentstruct].nonzero()[0]
        possibleprobs=numpy.squeeze(allprobstructs_NextProb[currentstruct][possiblewords,0].toarray(),axis=1)
        currentword=numpy.random.choice(possiblewords, p=possibleprobs)
        sampletext.append(currentword)
    # Return the created text
    return(sampletext)



%--------------------



#%% Build the n-grams for Sherlock Holmes   

# We have previously processed Sherlock Holmes as in lecture 2,
# so that myindices_in_vocabularies[0] is a list of vocabulary
# indices of words in order of appearance in Sherlock Holmes

# Build a 5-gram model
maxN=5
myngram=build_ngram(myindices_in_vocabularies[0],len(myvocabularies[0]),maxN)


# This can be an array of vocabulary indices of previously observed words 
initialtext=[]

# Sample a vector of word indices from the 5-gram 
# following the initial text
n_words_to_sample=100
sampledtext=sample_ngram(myngram,n_words_to_sample,maxN,initialtext)
# Print the result
' '.join(myvocabularies[0][sampledtext])



%--------------------



import nltk
import nltk.lm

# Create some example text documents as lists of their words
mytext=[['this','is','a','portion','of','text'],\ 
        ['this','is','another','portion'],\ 
        ['and','here','is','a','third','piece','of','text'],\
        ['this','is','clearly','a','fourth','piece'],\
        ['but','this','is','not'],\ 
        ['or','is','it','maybe','after','all'],\
        ['i','think','it','is'],\
        ['these','are','all','just','example','text','documents']]

# Create N-gram training data
maxN = 3
mynltk_ngramtraining_data, mynltk_padded_sentences = nltk.lm.preprocessing.padded_everygram_pipeline(maxN, mytext)

# Create the maximum-likelihood n-gram estimate
my_nltk_ngrammodel = nltk.lm.MLE(maxN) 
my_nltk_ngrammodel.fit(mynltk_ngramtraining_data, mynltk_padded_sentences)



%--------------------



# Count the number of unigrams 'this'
my_nltk_ngrammodel.counts['this']
Out[143]: 4

# Count the number of bigrams 'this is'
my_nltk_ngrammodel.counts[['this']]['is']
Out[144]: 4

# Count the number of trigrams 'this is a'
my_nltk_ngrammodel.counts[['this','is']]['a']
Out[145]: 1

# Probability of 'clearly' following 'this is'
my_nltk_ngrammodel.score('clearly',['this','is'])
Out[146]: 0.25

# Generate 10 words after 'this'
my_nltk_ngrammodel.generate(num_words=10,text_seed=['this'])
Out[148]: ['is', 'a', 'third', 'piece', 'of', 'text', '</s>', '</s>', '</s>', '</s>']



%--------------------



# Create a list of each character in order of appearance in Sherlock Holmes 
temptext=' '.join(mycrawled_lowercasetexts[0])
temptext=list(temptext)
# Create the "vocabulary" of the different individual characters
tempvocabulary=[]
myindices_in_tempvocabulary=[]
# Find the vocabulary of each document.
# Get unique words and where they occur
uniqueresults=numpy.unique(temptext,return_inverse=True)
tempvocabulary=uniqueresults[0]
tempindices=uniqueresults[1]

# Build the n-gram
maxN=5
myngram=build_ngram(tempindices,len(tempvocabulary),maxN)
# sample from the result
n_words_to_sample=800
initialtext=[]
sampledtext=sample_ngram(myngram,n_words_to_sample,maxN,initialtext)
''.join(tempvocabulary[sampledtext])

