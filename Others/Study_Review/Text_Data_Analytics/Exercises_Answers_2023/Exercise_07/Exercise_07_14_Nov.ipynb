{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6779cba8",
   "metadata": {},
   "source": [
    "# Ahmad Sharif\n",
    "***K436765***\n",
    "\n",
    "\n",
    "***DATA.STAT.840 Statistical Methods for Text Data Analysis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca13bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, hmmlearn, hmmlearn.hmm\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d78e858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MultinomialHMM has undergone major changes. The previous version was implementing a CategoricalHMM (a special case of MultinomialHMM). This new implementation follows the standard definition for a Multinomial distribution (e.g. as in https://en.wikipedia.org/wiki/Multinomial_distribution). See these issues for details:\n",
      "https://github.com/hmmlearn/hmmlearn/issues/335\n",
      "https://github.com/hmmlearn/hmmlearn/issues/340\n",
      "Even though the 'startprob_' attribute is set, it will be overwritten during initialization because 'init_params' contains 's'\n",
      "Even though the 'transmat_' attribute is set, it will be overwritten during initialization because 'init_params' contains 't'\n",
      "Fitting a model with 99 free scalar parameters with only 7 data points will result in a degenerate solution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1      -0.00000000             +nan\n",
      "         2       0.00000000      +0.00000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Exercise 7.1: The Forward-Backward algorithm.\n",
    "Consider a HMM model with the following vocabulary of 14 words: '''\n",
    "\n",
    "vocabulary = ['a', 'the', 'over', 'beside', 'near', 'quick', 'brown', 'lazy', 'jumps', 'runs', 'walks', 'fox', 'dog', 'cat']\n",
    "states = np.arange(1, 6)\n",
    "\n",
    "concatenated_data = ['the', 'quick', 'fox', 'jumps', 'over', 'a', 'dog']\n",
    "\n",
    "sequence = np.array([vocabulary.index(word) + 1 for word in concatenated_data]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=10, n_iter=100, verbose=True)\n",
    "model.startprob_ = np.array([1 / len(states)] * len(states))\n",
    "model.transmat_ = np.array([[0.2, 0.2, 0.2, 0.2, 0.2]] * len(states))\n",
    "model.emissionprob_ = np.array([[1.0] * len(vocabulary)] * len(states))\n",
    "\n",
    "\n",
    "model.fit(sequence)\n",
    "\n",
    "log_likelihood, state_sequence = model.score_samples(sequence)\n",
    "probability = np.exp(log_likelihood)\n",
    "\n",
    "print('Probability:', probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f7d97",
   "metadata": {},
   "source": [
    "***Exercise 7.4: Argue with an AI chatbot about hidden Markov models***\n",
    "\n",
    "I: Hidden Markov models and the Viterbi and Baum-Welch algorithms are still important and\n",
    "needed in the age of neural large language models.\n",
    "\n",
    "\n",
    "Do you agree ?\n",
    "\n",
    "AI: Yes, HMMs and their associated algorithms, such as the Viterbi and Baum-Welch algorithms, are still important and needed in the age of neural large language models. They provide interpretability, are effective for limited data and time-series analysis, allow for model training from unlabeled data, and can serve as a baseline model or for data generation. Their relevance depends on the specific task requirements and available resources.\n",
    "\n",
    "\n",
    "\n",
    "I : Markov model is better because probability of an occurring event is dependent only on the current state of a system. \n",
    "\n",
    "Are you still firm in your opinion\n",
    "\n",
    "\n",
    "Both Markov models and neural large language models have their own strengths and are useful for different tasks. Markov models excel in capturing dependencies between events based on the current state of the system, while neural large language models can capture complex patterns and long-range context. The choice depends on the specific task and data characteristics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
