{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6779cba8",
   "metadata": {},
   "source": [
    "# Ahmad Sharif\n",
    "***K436765***\n",
    "\n",
    "\n",
    "***DATA.STAT.840 Statistical Methods for Text Data Analysis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca13bb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ahmad/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import scipy.stats\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# (a) Download the 20 Newsgroups data set from http://qwone.com/~jason/20Newsgroups/.\n",
    "from sklearn.datasets import fetch_20newsgroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09251881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.31839701  5.69499476  4.75723351  4.44560893  4.33675234  4.169413\n",
      "  4.09657462  3.96486213  3.85453629  3.83934871]\n",
      "[0.01759519 0.02251091 0.00257834 ... 0.01100637 0.00228007 0.01042266]\n",
      "['edu' 'com' 'writes' 'ca' 'article' 'game' 'subject' 'don' 'organization'\n",
      " 'lines' 'car' 'like' 'university' 'just' 'team' 'posting' 'nntp' 'host'\n",
      " 'year' 'think']\n"
     ]
    }
   ],
   "source": [
    "''' b) In this exercise we consider only four of the newsgroups: rec.autos, rec.motorcycles,\n",
    "rec.sport.baseball, and rec.sport.hockey. Process the documents of the four newsgroups\n",
    "using the pipeline described on the lectures, including vocabulary pruning. '''\n",
    "\n",
    "categories = ['rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey']\n",
    "dataset = fetch_20newsgroups(subset='all', shuffle=True, random_state=42, categories=categories)\n",
    "corpus = dataset.data\n",
    "\n",
    "\n",
    "dataset\n",
    "\n",
    "''' c) Create a TF-IDF representation for the documents, using Length-normalized frequency (TF)\n",
    "and Smoothed logarithmic inverse document frequency (IDF). '''\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Vocabulary pruning using LSA\n",
    "n_low_dimensions = 10\n",
    "lsa_model = TruncatedSVD(n_components=n_low_dimensions)\n",
    "X_small = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "\n",
    "print(lsa_model.singular_values_)\n",
    "\n",
    "# Examine a factor (here the one with the largest singular value)\n",
    "print(lsa_model.components_[0, :])\n",
    "\n",
    "# 20 words with the largest absolute weights in the factor\n",
    "top_weights_indices = np.argsort(-1 * np.abs(lsa_model.components_[0, :]))\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "print(feature_names[top_weights_indices[0:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b129b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "adding a nonzero scalar to a sparse matrix is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m n_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     54\u001b[0m n_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 55\u001b[0m pi, psi, theta \u001b[38;5;241m=\u001b[39m plsa(tfidf_matrix, n_topics, n_iterations)\n",
      "Cell \u001b[0;32mIn[3], line 36\u001b[0m, in \u001b[0;36mplsa\u001b[0;34m(document_to_word_matrix, n_topics, n_iterations)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_topics):\n\u001b[1;32m     35\u001b[0m     psi[t, :] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39marray(np\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m---> 36\u001b[0m         np\u001b[38;5;241m.\u001b[39mmultiply(document_to_word_matrix \u001b[38;5;241m+\u001b[39m epsilon, doc_word_to_topics[t]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     37\u001b[0m psi \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(np\u001b[38;5;241m.\u001b[39msum(psi, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), (n_topics, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_topics):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/sparse/_base.py:467\u001b[0m, in \u001b[0;36mspmatrix.__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;66;03m# Now we would add this scalar to every element.\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madding a nonzero scalar to a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    468\u001b[0m                               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse matrix is not supported\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m isspmatrix(other):\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape:\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: adding a nonzero scalar to a sparse matrix is not supported"
     ]
    }
   ],
   "source": [
    "'''  c) Create a TF-IDF representation for the documents, using Length-normalized frequency (TF)\n",
    "and Smoothed logarithmic inverse document frequency (IDF). '''\n",
    "\n",
    "\n",
    "def plsa(document_to_word_matrix, n_topics, n_iterations):\n",
    "    n_docs, n_vocab = np.shape(document_to_word_matrix)\n",
    "\n",
    "   \n",
    "    theta = np.random.uniform(size=(n_vocab, n_topics))\n",
    "    theta /= np.tile(np.sum(theta, axis=0), (n_vocab, 1))\n",
    "\n",
    "    psi = np.random.uniform(size=(n_topics, n_docs))\n",
    "    psi /= np.tile(np.sum(psi, axis=0), (n_topics, 1))\n",
    "\n",
    "    n_words_in_docs = np.squeeze(np.array(np.sum(document_to_word_matrix, axis=1)))\n",
    "    n_total_words = np.sum(n_words_in_docs)\n",
    "    pi = n_words_in_docs / n_total_words\n",
    "\n",
    "    for _ in range(n_iterations):\n",
    "        # E-step\n",
    "        doc_word_to_topics = []\n",
    "        doc_word_to_topic_sum = np.zeros((n_docs, n_vocab))\n",
    "        for t in range(n_topics):\n",
    "            doc_word_to_topict = np.tile(theta[:, t], (n_docs, 1)) * np.tile(psi[t, :], (n_vocab, 1)).T\n",
    "            epsilon = 1e-14\n",
    "            doc_word_to_topict += epsilon\n",
    "            doc_word_to_topics.append(doc_word_to_topict)\n",
    "            doc_word_to_topic_sum += doc_word_to_topict\n",
    "\n",
    "        for t in range(n_topics):\n",
    "            doc_word_to_topics[t] /= doc_word_to_topic_sum\n",
    "\n",
    "        # M-step\n",
    "        for t in range(n_topics):\n",
    "            psi[t, :] = np.squeeze(np.array(np.sum(\n",
    "                np.multiply(document_to_word_matrix + epsilon, doc_word_to_topics[t]), axis=1)))\n",
    "        psi /= np.tile(np.sum(psi, axis=0), (n_topics, 1))\n",
    "\n",
    "        for t in range(n_topics):\n",
    "            theta[:, t] = np.squeeze(np.array(np.sum(\n",
    "                np.multiply(document_to_word_matrix, doc_word_to_topics[t]), axis=0).T))\n",
    "        theta /= np.tile(np.sum(theta, axis=0), (n_vocab, 1))\n",
    "\n",
    "    return pi, psi, theta\n",
    "\n",
    "\n",
    "# TF-IDF Vectorization with Length-normalized frequency (TF) and Smoothed logarithmic IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "tfidf_matrix = csr_matrix(tfidf_matrix)  # Convert to sparse matrix\n",
    "\n",
    "# PLSA\n",
    "n_topics = 10\n",
    "n_iterations = 50\n",
    "pi, psi, theta = plsa(tfidf_matrix, n_topics, n_iterations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9c7bdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.31839701  5.69499716  4.75716961  4.44274412  4.337713    4.17673975\n",
      "  4.09626038  3.96767267  3.85081661  3.82714268]\n",
      "[[ 0.01759533  0.02251085  0.00257856 ...  0.01100638  0.00228009\n",
      "   0.01042255]\n",
      " [ 0.00541773 -0.01817195 -0.00504579 ...  0.00788033  0.00504621\n",
      "  -0.02207526]\n",
      " [ 0.00695189 -0.0215263   0.00444565 ...  0.00614491  0.00564106\n",
      "   0.01453824]\n",
      " ...\n",
      " [-0.02312671 -0.00900819 -0.00438567 ...  0.00242274 -0.00435804\n",
      "   0.02864854]\n",
      " [ 0.01498695  0.02171728  0.00140785 ... -0.00126983  0.00900423\n",
      "  -0.03144969]\n",
      " [-0.01192349  0.01080683 -0.01545888 ... -0.00283797  0.00045932\n",
      "   0.05358575]]\n",
      "[[ 0.14863628  0.10713103  0.11191805 ... -0.04761222  0.05171661\n",
      "   0.00891934]\n",
      " [ 0.17835921 -0.03078103 -0.07558104 ... -0.07305895 -0.11017075\n",
      "  -0.11658226]\n",
      " [ 0.22019046  0.12428973  0.01533644 ...  0.07663957 -0.09047817\n",
      "  -0.01684534]\n",
      " ...\n",
      " [ 0.12762034 -0.01895873  0.02862286 ... -0.02250516  0.0141198\n",
      "  -0.00151372]\n",
      " [ 0.11245082  0.04849857  0.04339154 ... -0.0069751  -0.0078708\n",
      "   0.0091013 ]\n",
      " [ 0.0998395  -0.05067188 -0.00074359 ...  0.07076911  0.09786765\n",
      "  -0.02503128]]\n"
     ]
    }
   ],
   "source": [
    "''' d) Apply latent semantic analysis to the TF-IDF matrix, to find 10 underlying factors. '''\n",
    "\n",
    "\n",
    "\n",
    "n_topics = 10\n",
    "\n",
    "# Apply TruncatedSVD for LSA\n",
    "lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "lsa_matrix = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Examine the factors\n",
    "print(lsa_model.singular_values_)  # Singular values\n",
    "print(lsa_model.components_)        # Factors\n",
    "\n",
    "# Access the transformed TF-IDF\n",
    "print(lsa_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f79353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor 1:\n",
      "['edu' 'com' 'writes' 'ca' 'article' 'game' 'subject' 'don' 'organization'\n",
      " 'lines']\n",
      "\n",
      "Factor 2:\n",
      "['com' 'game' 'car' 'team' 'bike' 'hockey' 'games' 'sun' 'espn' 'win']\n",
      "\n",
      "Factor 3:\n",
      "['edu' 'com' 'sun' 'ca' 'east' 'car' 'state' 'ohio' 'university' 'ed']\n",
      "\n",
      "Factor 4:\n",
      "['sun' 'car' 'edu' 'east' 'ed' 'green' 'columbia' 'gld' 'espn' 'egreen']\n",
      "\n",
      "Factor 5:\n",
      "['ca' 'com' 'baseball' 'car' 'edu' 'sun' 'bnr' 'jewish' 'braves' 'east']\n",
      "\n",
      "Factor 6:\n",
      "['ca' 'car' 'cs' 'maynard' 'roger' 'bike' 'laurentian' 'sun' 'espn' 'com']\n",
      "\n",
      "Factor 7:\n",
      "['sun' 'nec' 'behanna' 'east' 'car' 'green' 'nj' 'ed' 'com' 'maynard']\n",
      "\n",
      "Factor 8:\n",
      "['columbia' 'gld' 'espn' 'cunixb' 'cc' 'dare' 'gary' 'game' 'ohio'\n",
      " 'andrew']\n",
      "\n",
      "Factor 9:\n",
      "['columbia' 'gld' 'cmu' 'game' 'cc' 'andrew' 'cunixb' 'hp' 'behanna' 'nec']\n",
      "\n",
      "Factor 10:\n",
      "['behanna' 'nec' 'ohio' 'state' 'magnus' 'cmu' 'andrew' 'acs' 'nj' 'syl']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' e) (e) Describe the resulting factors: list the 10 words with highest (absolute) weight in each\n",
    "factor. Do the factors seem related to individual newsgroups? Does their content seem\n",
    "meaningful? '''\n",
    "\n",
    "\n",
    "# Get the feature names from the TF-IDF vectorizer\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Get the words with the highest absolute weight in each factor\n",
    "for i in range(n_topics):\n",
    "    factor_weights = lsa_model.components_[i]\n",
    "    top_word_indices = np.argsort(np.abs(factor_weights))[::-1][:10]\n",
    "    top_words = feature_names[top_word_indices]\n",
    "    \n",
    "    print(f\"Factor {i + 1}:\")\n",
    "    print(top_words)\n",
    "    print()\n",
    "    \n",
    "# It seems meaningful. However, some nouns are dominant tokens like 'cunixb'.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74b4e423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factor 1:\n",
      "['edu' 'com' 'writes' 'ca' 'article' 'game' 'subject' 'don' 'organization'\n",
      " 'lines' 'car' 'like' 'university' 'just' 'team']\n",
      "\n",
      "Factor 2:\n",
      "['com' 'game' 'car' 'team' 'bike' 'hockey' 'games' 'sun' 'espn' 'win'\n",
      " 'dod' 'players' 'play' 'season' 'year']\n",
      "\n",
      "Factor 3:\n",
      "['edu' 'com' 'sun' 'ca' 'east' 'car' 'state' 'ohio' 'university' 'ed'\n",
      " 'andrew' 'cc' 'green' 'team' 'uiuc']\n",
      "\n",
      "Factor 4:\n",
      "['sun' 'car' 'edu' 'east' 'ed' 'green' 'columbia' 'gld' 'espn' 'egreen'\n",
      " 'cc' 'cmu' 'andrew' 'year' 'buffalo']\n",
      "\n",
      "Factor 5:\n",
      "['ca' 'com' 'baseball' 'car' 'edu' 'sun' 'bnr' 'jewish' 'braves' 'east'\n",
      " 'hockey' 'canada' 'runs' 'bike' 'netcom']\n",
      "\n",
      "Factor 6:\n",
      "['ca' 'car' 'cs' 'maynard' 'roger' 'bike' 'laurentian' 'sun' 'espn' 'com'\n",
      " 'edu' 'game' 'ramsey' 'duke' 'dod']\n",
      "\n",
      "Factor 7:\n",
      "['sun' 'nec' 'behanna' 'east' 'car' 'green' 'nj' 'ed' 'com' 'maynard'\n",
      " 'syl' 'egreen' 'bike' 'ca' '11']\n",
      "\n",
      "Factor 8:\n",
      "['columbia' 'gld' 'espn' 'cunixb' 'cc' 'dare' 'gary' 'game' 'ohio'\n",
      " 'andrew' 'magnus' 'state' 'cmu' 'behanna' 'nec']\n",
      "\n",
      "Factor 9:\n",
      "['columbia' 'gld' 'cmu' 'game' 'cc' 'andrew' 'cunixb' 'hp' 'behanna' 'nec'\n",
      " 'dare' 'gary' 'espn' 'pens' 'games']\n",
      "\n",
      "Factor 10:\n",
      "['behanna' 'nec' 'ohio' 'state' 'magnus' 'cmu' 'andrew' 'acs' 'nj' 'syl'\n",
      " 'sun' 'chris' 'bnr' 'uk' 'list']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' (f) Do the same with 15 factors (the first 10 factors will be the same). Do the new 5 factors\n",
    "seem more or less meaningful? '''\n",
    "\n",
    "# Get the feature names from the TF-IDF vectorizer\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Get the words with the highest absolute weight in each factor\n",
    "for i in range(n_topics):\n",
    "    factor_weights = lsa_model.components_[i]\n",
    "    top_word_indices = np.argsort(np.abs(factor_weights))[::-1][:15]\n",
    "    top_words = feature_names[top_word_indices]\n",
    "    \n",
    "    print(f\"Factor {i + 1}:\")\n",
    "    print(top_words)\n",
    "    print()\n",
    "    \n",
    "# More meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e4ac9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "''' (a) Using the same data as in Exercise 6.1 (four newsgroups), create a term frequency matrix of\n",
    "raw term counts for the documents. '''\n",
    "\n",
    "corpus = dataset.data\n",
    "\n",
    "# Create the CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "\n",
    "# Fit and transform the documents to get the term frequency matrix\n",
    "tf_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix if needed\n",
    "tf_matrix_dense = tf_matrix.todense()\n",
    "\n",
    "# Display the term frequency matrix\n",
    "print(tf_matrix_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b574d327",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m''' c) Describe the resulting factors: list the 10 words with highest probability in each factor. '''\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m top_words_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m theta, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Print the top 10 words for each factor\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_topics_plsa):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theta' is not defined"
     ]
    }
   ],
   "source": [
    "''' c) Describe the resulting factors: list the 10 words with highest probability in each factor. '''\n",
    "\n",
    "top_words_indices = np.argsort(-1 * theta, axis=0)\n",
    "\n",
    "# Print the top 10 words for each factor\n",
    "for i in range(n_topics_plsa):\n",
    "    print(f\"Factor {i + 1}:\")\n",
    "    top_words = feature_names[top_words_indices[:, i]][:10]\n",
    "    print(\", \".join(top_words))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93e5fbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_matrix :    (0, 1995)\t3\n",
      "  (0, 2276)\t3\n",
      "  (0, 4721)\t5\n",
      "  (0, 905)\t5\n",
      "  (0, 2090)\t2\n",
      "  (0, 670)\t2\n",
      "  (0, 4340)\t1\n",
      "  (0, 3461)\t1\n",
      "  (0, 2969)\t1\n",
      "  (0, 4260)\t3\n",
      "  (0, 3163)\t1\n",
      "  (0, 3496)\t1\n",
      "  (0, 2255)\t1\n",
      "  (0, 3749)\t1\n",
      "  (0, 3275)\t1\n",
      "  (0, 4681)\t1\n",
      "  (0, 4766)\t2\n",
      "  (0, 706)\t1\n",
      "  (0, 938)\t1\n",
      "  (0, 2717)\t1\n",
      "  (0, 180)\t1\n",
      "  (0, 585)\t2\n",
      "  (0, 3168)\t2\n",
      "  (0, 1830)\t2\n",
      "  (0, 4946)\t2\n",
      "  :\t:\n",
      "  (3978, 2947)\t1\n",
      "  (3978, 1005)\t1\n",
      "  (3978, 3683)\t1\n",
      "  (3978, 2189)\t1\n",
      "  (3978, 1055)\t1\n",
      "  (3978, 2465)\t1\n",
      "  (3978, 3318)\t1\n",
      "  (3978, 3818)\t1\n",
      "  (3978, 2136)\t1\n",
      "  (3978, 87)\t1\n",
      "  (3978, 346)\t1\n",
      "  (3978, 3341)\t1\n",
      "  (3978, 1156)\t1\n",
      "  (3978, 3837)\t1\n",
      "  (3978, 784)\t1\n",
      "  (3978, 1410)\t1\n",
      "  (3978, 2475)\t1\n",
      "  (3978, 2164)\t1\n",
      "  (3978, 4919)\t1\n",
      "  (3978, 3920)\t1\n",
      "  (3978, 886)\t1\n",
      "  (3978, 3663)\t1\n",
      "  (3978, 1332)\t1\n",
      "  (3978, 986)\t1\n",
      "  (3978, 4986)\t3\n",
      "tf_matrix_dense :  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "''' 6.3.1 (a)create a term frequency matrix of raw term counts for the documents. '''\n",
    "\n",
    "corpus = dataset.data  # This line is already provided in your code\n",
    "\n",
    "# Create a CountVectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english', max_features=5000)  # You can adjust max_features as needed\n",
    "\n",
    "# Fit and transform the documents to obtain the term frequency matrix\n",
    "tf_matrix = count_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the feature names (words) corresponding to the columns of the matrix\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the sparse matrix to a dense matrix if needed\n",
    "tf_matrix_dense = tf_matrix.toarray()\n",
    "print('tf_matrix : ', tf_matrix)\n",
    "print('tf_matrix_dense : ', tf_matrix_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "516eac9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'matlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m n_iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Apply PLSA to the term frequency matrix\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m pi_plsa, psi_plsa, theta_plsa \u001b[38;5;241m=\u001b[39m plsa(tf_matrix_dense, n_topics, n_iterations)\n",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m, in \u001b[0;36mplsa\u001b[0;34m(document_to_word_matrix, n_topics, n_iterations)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize theta and psi with random values\u001b[39;00m\n\u001b[1;32m      9\u001b[0m theta \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(n_vocab, n_topics)\n\u001b[0;32m---> 10\u001b[0m theta \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatlib\u001b[38;5;241m.\u001b[39mrepmat(np\u001b[38;5;241m.\u001b[39msum(theta, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), n_vocab, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m psi \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(n_topics, n_docs)\n\u001b[1;32m     13\u001b[0m psi \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatlib\u001b[38;5;241m.\u001b[39mrepmat(np\u001b[38;5;241m.\u001b[39msum(psi, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), n_topics, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/numpy/__init__.py:320\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tester\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'matlib'"
     ]
    }
   ],
   "source": [
    "\n",
    "''' b) Apply Latent Dirichlet Allocation to the term frequency matrix to find 10 underlying topic. '''\n",
    "\n",
    "n_topics = 10\n",
    "n_iterations = 200\n",
    "\n",
    "\n",
    "pi_plsa, psi_plsa, theta_plsa = plsa(tf_matrix_dense, n_topics, n_iterations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da7da51e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta_plsa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m''' c) '''\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Assuming 'feature_names' is the array of feature names obtained from the CountVectorizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# and 'theta_plsa' is the matrix obtained from PLSA\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get the indices of the top words for each factor\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m top_words_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m theta_plsa, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Print the top 10 words for each topic\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_topics):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theta_plsa' is not defined"
     ]
    }
   ],
   "source": [
    "''' c) Describe the resulting factors: list the 10 words with highest probability in each topic. ''' \n",
    "# Assuming 'feature_names' is the array of feature names obtained from the CountVectorizer\n",
    "# and 'theta_plsa' is the matrix obtained from PLSA\n",
    "\n",
    "# Get the indices of the top words for each factor\n",
    "top_words_indices = np.argsort(-1 * theta_plsa, axis=0)\n",
    "\n",
    "# Print the top 10 words for each topic\n",
    "for i in range(n_topics):\n",
    "    print(f\"Topic {i + 1}:\")\n",
    "    top_words = feature_names[top_words_indices[:, i]][:10]\n",
    "    print(\", \".join(top_words))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b30d28a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'psi_plsa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m''' d) '''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming 'psi_plsa' is the matrix obtained from PLSA\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# and 'corpus' is the list of documents\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Get the index of the document with the highest probability for each topic\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m top_doc_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(psi_plsa, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Print the first 100 words of the top document for each topic\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(top_doc_indices):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'psi_plsa' is not defined"
     ]
    }
   ],
   "source": [
    "''' d) Find, for each topic, the document (message) with highest probability of that topic, and print\n",
    "its 100 first words. '''\n",
    "\n",
    "# Assuming 'psi_plsa' is the matrix obtained from PLSA\n",
    "# and 'corpus' is the list of documents\n",
    "\n",
    "# Get the index of the document with the highest probability for each topic\n",
    "top_doc_indices = np.argmax(psi_plsa, axis=1)\n",
    "\n",
    "# Print the first 100 words of the top document for each topic\n",
    "for i, doc_index in enumerate(top_doc_indices):\n",
    "    print(f\"Topic {i + 1} - Document with Highest Probability:\")\n",
    "    top_doc = corpus[doc_index]\n",
    "    # Print the first 100 words\n",
    "    print(\" \".join(top_doc.split()[:100]))\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
