{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6779cba8",
   "metadata": {},
   "source": [
    "# Ahmad Sharif\n",
    "***K436765***\n",
    "\n",
    "***Exercise 11***\n",
    "\n",
    "***DATA.STAT.840 Statistical Methods for Text Data Analysis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d78e858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Scores:\n",
      "Similarity with d1: 0.40073573105287036\n",
      "Similarity with d2: 0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "d1 = \"the robot is insightful but you are strong and i may answer and the wise fox is insightful and you are insightful and i am insightful but i will explain the insightful bird\"\n",
    "d2 = \"the bird is insightful\"\n",
    "query = \"insightful bird\"\n",
    "\n",
    "documents = [d1, d2]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "query_vector = tfidf_vectorizer.transform([query])\n",
    "\n",
    "similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "print(\"Cosine Similarity Scores:\")\n",
    "print(f\"Similarity with d1: {similarity_scores[0][0]}\")\n",
    "print(f\"Similarity with d2: {similarity_scores[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87da4e",
   "metadata": {},
   "source": [
    "d2 is more close compare to d1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78414eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unigram Probabilities:\n",
      "Probability of query given d1: 0.004591368227731864\n",
      "Probability of query given d2: 0.0625\n"
     ]
    }
   ],
   "source": [
    "tokens_d1 = d1.split()\n",
    "tokens_d2 = d2.split()\n",
    "\n",
    "all_tokens = tokens_d1 + tokens_d2\n",
    "\n",
    "word_count_d1 = Counter(tokens_d1)\n",
    "word_count_d2 = Counter(tokens_d2)\n",
    "word_count_all = Counter(all_tokens)\n",
    "\n",
    "query_tokens = query.split()\n",
    "\n",
    "prob_d1 = 1.0\n",
    "prob_d2 = 1.0\n",
    "\n",
    "for token in query_tokens:\n",
    "    prob_d1 *= (word_count_d1[token] / len(tokens_d1)) if token in tokens_d1 else 0\n",
    "    prob_d2 *= (word_count_d2[token] / len(tokens_d2)) if token in tokens_d2 else 0\n",
    "\n",
    "print(\"\\nUnigram Probabilities:\")\n",
    "print(f\"Probability of query given d1: {prob_d1}\")\n",
    "print(f\"Probability of query given d2: {prob_d2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a8ce6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank Prior Probabilities:\n",
      "d1: 0.2972101275128807\n",
      "d2: 0.23343473174786417\n",
      "d3: 0.163813582448939\n",
      "d4: 0.30554155829031626\n"
     ]
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_edges_from([('d1', 'd2'), ('d1', 'd3'), ('d2', 'd4'), ('d3', 'd2'), ('d3', 'd4'), ('d4', 'd1')])\n",
    "\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "print(\"PageRank Prior Probabilities:\")\n",
    "for document, probability in pagerank.items():\n",
    "    print(f\"{document}: {probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62515200",
   "metadata": {},
   "source": [
    "d1 and d4 have top page rank and very close each other. d2 and d3 between 0.16-0.23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c242342",
   "metadata": {},
   "source": [
    "***Exercise 10.3: Limitations and improvements of Pagerank.\n",
    "a) Ask a large language model (see previous exercises for some possibilities) two questions: 1. what\n",
    "the limitations of the Pagerank algorithm are, and 2. in what ways the algorithm can be improved.***\n",
    "\n",
    "\n",
    "1. The limitations of the Pagerank algorithm include:\n",
    "- Dependency on the web graph: The algorithm relies on the structure of the web graph, and therefore it may not work well for non-web applications or in cases where the graph is not easily accessible.\n",
    "- Susceptible to manipulation: The Pagerank algorithm can be manipulated by creating artificial links or by \"link spamming\" to inflate the importance of certain web pages.\n",
    "- Lack of freshness: Pagerank does not consider the freshness or recency of web pages, and therefore it may not accurately reflect the most up-to-date information.\n",
    "- Lack of personalization: Pagerank treats all users equally and does not consider individual preferences or personalized relevance.\n",
    "\n",
    "2. The Pagerank algorithm can be improved in several ways:\n",
    "- Considering additional features: Enhancing the algorithm by incorporating additional features such as textual content, user behavior, or social signals can provide more accurate rankings and improve relevance.\n",
    "- Time-sensitive factors: Incorporating time-sensitive factors and considering the freshness of web pages can help in providing more up-to-date results.\n",
    "- Personalization: Introducing personalization into the algorithm by considering user preferences, browsing history, or demographic information can enhance the relevance of the ranked results.\n",
    "- Robustness against manipulation: Developing techniques to make the Pagerank algorithm more robust against manipulation attempts, such as link spamming or artificial link creation, can improve the quality and fairness of rankings.\n",
    "- Incorporating user feedback: Incorporating feedback mechanisms to allow users to influence the rankings based on their judgments or ratings can improve the algorithm's effectiveness and user satisfaction.\n",
    "\n",
    "\n",
    "\n",
    "Reference: https://deepai.org/chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c155bd1e",
   "metadata": {},
   "source": [
    "***b) Find either from the course material or online material information that would support/agree with\n",
    "the answers that the large language model gave (or information that disputes/disagrees with the\n",
    "answers), and modify the answers to add the supporting/disputing information, with references to\n",
    "where you found it. Note: you do not need to add a massive amount of additional information, but\n",
    "try to add something for at least most of the points the large language model makes.***\n",
    "\n",
    "\n",
    "\n",
    "Limitations are more or less correct. Still it missed some points. Like spaming and people often do it with money. Even the improvement it suggests do not work in my opinion. Because nowadays people can do the mentioned things from any Marketing company. So I dont think so it work.\n",
    "\n",
    "\n",
    "\n",
    "Reference: \n",
    "https://medium.com/@sarthakanand/page-rank-b7072c61dd85\n",
    "https://medium.com/polo-club-of-data-science/pagerank-algorithm-explained-with-examples-a5e25e2594c9\n",
    "https://towardsdatascience.com/pagerank-algorithm-fully-explained-dc794184b4af"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
