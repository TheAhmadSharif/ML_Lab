{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca520f9d-8679-41e7-af42-9e31c145f4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation_name': 'sigmoid',\n",
      " 'category': 'RG',\n",
      " 'classification_type': 'RG',\n",
      " 'directory_path': 'Classified_Dataset/',\n",
      " 'folder_name': '001_R_RG_Random_EPOCH_120',\n",
      " 'get_filename': '001_R_RG_Random_Summary.csv',\n",
      " 'loss_function_name': 'binary_crossentropy',\n",
      " 'name': 'RG_001',\n",
      " 'set_img_size': (250, 250),\n",
      " 'test_category': 'RG::RG::Random::120'}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index_value = 0 ########################################################################### Just Change it ####################\n",
    "get_img_size = (250, 250)\n",
    "epoch_number = 120\n",
    "loop_number = 1\n",
    "\n",
    "from pprint import pprint \n",
    "\n",
    "# Initialize an empty list to hold the configurations\n",
    "configurations = []\n",
    "category = \"Muse\"\n",
    "i = 0\n",
    "\n",
    "classification_type = \"RG\"\n",
    "loss_function_name = 'binary_crossentropy'  # categorical_crossentropy\n",
    "activation_name = 'sigmoid' # softmax\n",
    "suffix = \"Random\"\n",
    "classification_type = \"RG\"\n",
    "category = \"RG\"\n",
    "category_initial = category[0]\n",
    "config = {\n",
    "    \"name\": f\"{category}_{i + 1:03}\",  # Example name format: \"Muse_001\", \"Unicorn_005\", etc.\n",
    "    \"category\": category,\n",
    "    \"classification_type\": classification_type,\n",
    "    \"set_img_size\": get_img_size,\n",
    "    \"get_filename\": f\"{i + 1:03}_{category_initial}_{classification_type}_{suffix}_Summary.csv\",\n",
    "    \"folder_name\": f\"{i + 1:03}_{category_initial}_{classification_type}_{suffix}_EPOCH_{epoch_number}\",\n",
    "    \"test_category\": f\"{category}::{classification_type}::{suffix}::{epoch_number}\",\n",
    "    \"directory_path\": \"Classified_Dataset/\",\n",
    "    \"loss_function_name\": loss_function_name,\n",
    "    \"activation_name\": activation_name\n",
    "}\n",
    "\n",
    "configurations.append(config)\n",
    "pprint(configurations[index_value])\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28283d9e-cf37-4d52-ab85-4b2e6dd15c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 10:41:42.561066: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-03 10:41:42.728634: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-03 10:41:42.890788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-03 10:41:43.045128: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-03 10:41:43.091548: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-03 10:41:43.310658: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-03 10:41:45.093105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 365 images with corresponding labels.\n",
      "\n",
      "Random Split 1/10: Running model training and evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 596ms/step - accuracy: 0.7449 - loss: 1.3025\n",
      "Test accuracy for split 1: 0.7397\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 645ms/step\n",
      "Confusion Matrix for split 1:\n",
      "[[54  2]\n",
      " [17  0]]\n",
      "[0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0\n",
      " 0 1 1 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1] ____________ test_label\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] ____________ predicted_classes\n",
      "\n",
      "\n",
      "\n",
      "Sensitivity: 0.0000, Specificity: 0.9643\n",
      "\n",
      "Random Split 2/10: Running model training and evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 273ms/step - accuracy: 0.7439 - loss: 1.2349\n",
      "Test accuracy for split 2: 0.7534\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 308ms/step\n",
      "Confusion Matrix for split 2:\n",
      "[[55  0]\n",
      " [18  0]]\n",
      "[0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0\n",
      " 0 1 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0] ____________ test_label\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] ____________ predicted_classes\n",
      "\n",
      "\n",
      "\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Random Split 3/10: Running model training and evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 289ms/step - accuracy: 0.8358 - loss: 0.5303\n",
      "Test accuracy for split 3: 0.8356\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x740d777e5b20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 459ms/stepWARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x740d777e5b20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 311ms/step\n",
      "Confusion Matrix for split 3:\n",
      "[[61  0]\n",
      " [12  0]]\n",
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1] ____________ test_label\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] ____________ predicted_classes\n",
      "\n",
      "\n",
      "\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Random Split 4/10: Running model training and evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 272ms/step - accuracy: 0.7967 - loss: 1.4670\n",
      "Test accuracy for split 4: 0.7808\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 296ms/step\n",
      "Confusion Matrix for split 4:\n",
      "[[53  6]\n",
      " [10  4]]\n",
      "[1 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1] ____________ test_label\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0] ____________ predicted_classes\n",
      "\n",
      "\n",
      "\n",
      "Sensitivity: 0.2857, Specificity: 0.8983\n",
      "\n",
      "Random Split 5/10: Running model training and evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 271ms/step - accuracy: 0.8143 - loss: 0.6109\n",
      "Test accuracy for split 5: 0.8082\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 293ms/step\n",
      "Confusion Matrix for split 5:\n",
      "[[59  0]\n",
      " [14  0]]\n",
      "[0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0] ____________ test_label\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] ____________ predicted_classes\n",
      "\n",
      "\n",
      "\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "\n",
      "Random Split 6/10: Running model training and evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 268ms/step - accuracy: 0.7126 - loss: 1.4929\n",
      "Test accuracy for split 6: 0.6986\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 309ms/step\n",
      "Confusion Matrix for split 6:\n",
      "[[49  2]\n",
      " [20  2]]\n",
      "[1 0 0 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 1] ____________ test_label\n",
      "[0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] ____________ predicted_classes\n",
      "\n",
      "\n",
      "\n",
      "Sensitivity: 0.0909, Specificity: 0.9608\n",
      "\n",
      "Random Split 7/10: Running model training and evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmad/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "import numpy as np\n",
    "import os, cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "# Start timer\n",
    "start_time = datetime.now()\n",
    "\n",
    "####################  Parameters #########################\n",
    " \n",
    "n_splits = 10   ###############################################################################\n",
    "splitter_test_size = 0.2   \n",
    "\n",
    "get_config_size = configurations[index_value][\"set_img_size\"]\n",
    "total_feature_size = get_img_size + (1,)\n",
    "get_filename = configurations[index_value][\"get_filename\"]\n",
    "folder_name = configurations[index_value][\"folder_name\"]\n",
    "test_category = configurations[index_value][\"test_category\"]\n",
    "directory_path = configurations[index_value][\"directory_path\"]\n",
    "loss_function_name = configurations[index_value][\"loss_function_name\"]\n",
    "activation_name = configurations[index_value][\"activation_name\"]\n",
    "optimizer_name = 'adam'\n",
    "\n",
    "get_font = 'monospace'\n",
    "# Text annotation settings\n",
    "x_distance = 0.72\n",
    "y_distance = 0.30\n",
    "transparency_level = 0.4\n",
    "get_font_size = 9\n",
    "#################### Hyperparameters ####################\n",
    "validation_size = 0.2\n",
    "get_dropout_rate = 0.5\n",
    "filter_size_1 = 10\n",
    "filter_size_2 = 20\n",
    "get_batch_size = 200\n",
    "model_padding_size = 'same' \n",
    "model_kernel_size = (5,5)\n",
    "\n",
    "# Create folder to save results\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "# Initialize metrics lists\n",
    "accuracy_list = []\n",
    "sensitivity_list = []\n",
    "specificity_list = []\n",
    "\n",
    "# Function to save performance summary\n",
    "def save_performance_summary(epoch_number, sensitivity, specificity, accuracy_score):\n",
    "    directory = \"Excel_Files\"\n",
    "    filename = os.path.join(directory, get_filename)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write(\"Epochs,Sensitivity,Specificity,Accuracy,Date_Time\\n\")\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(filename, 'a') as file:\n",
    "        file.write(f\"{epoch_number},{sensitivity:.2f},{specificity:.2f},{accuracy_score:.2f},{current_time}\\n\")\n",
    "\n",
    "# Function to create the CNN model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=filter_size_1, kernel_size=model_kernel_size, padding=model_padding_size, input_shape=total_feature_size, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(get_dropout_rate))\n",
    "    model.add(Conv2D(filters=filter_size_2, kernel_size=model_kernel_size, padding=model_padding_size, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(get_dropout_rate))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(2, activation=activation_name))\n",
    "    return model\n",
    "\n",
    "# Load images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "Folder_1 = os.path.join(directory_path, 'Depuy')\n",
    "Folder_2 = os.path.join(directory_path, 'Tornier')\n",
    "\n",
    "# Helper function to load images and assign labels\n",
    "def getData(folder, label):\n",
    "    file_names = os.listdir(folder)\n",
    "    for file_name in file_names:\n",
    "        path = os.path.join(folder, file_name)\n",
    "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, dsize=get_config_size)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "\n",
    "# Load relaxation and game images\n",
    "getData(Folder_1, 0)  # Label 0 for Relaxation\n",
    "getData(Folder_2, 1)        # Label 1 for Game\n",
    "\n",
    "print(f\"Loaded {len(images)} images with corresponding labels.\")\n",
    "\n",
    "# ShuffleSplit configuration\n",
    "splitter = ShuffleSplit(n_splits = n_splits, test_size=splitter_test_size, random_state=42)\n",
    "for x in range(loop_number):\n",
    "    for split_num, (train_idx, test_idx) in enumerate(splitter.split(images)):\n",
    "        print(f\"\\nRandom Split {split_num + 1}/{n_splits}: Running model training and evaluation\")\n",
    "\n",
    "        # Prepare training and testing datasets\n",
    "        train_feature, test_feature = np.array(images)[train_idx], np.array(images)[test_idx]\n",
    "        train_label, test_label = np.array(labels)[train_idx], np.array(labels)[test_idx]\n",
    "\n",
    "        # Reshape and normalize\n",
    "        train_feature_vector = train_feature.reshape(-1, *total_feature_size).astype('float32') / 255\n",
    "        test_feature_vector = test_feature.reshape(-1, *total_feature_size).astype('float32') / 255\n",
    "        train_label_onehot = to_categorical(train_label, num_classes=2)\n",
    "        test_label_onehot = to_categorical(test_label, num_classes=2)\n",
    "\n",
    "        # Create and compile the model\n",
    "        model = create_model()\n",
    "        model.compile(loss=loss_function_name, optimizer=optimizer_name, metrics=['accuracy'])\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(train_feature_vector, train_label_onehot, \n",
    "                            validation_split=validation_size, \n",
    "                            epochs=epoch_number, \n",
    "                            batch_size=get_batch_size, verbose=0)\n",
    "\n",
    "        # Evaluate the model\n",
    "        scores = model.evaluate(test_feature_vector, test_label_onehot)\n",
    "        accuracy_score = scores[1]\n",
    "        print(f\"Test accuracy for split {split_num + 1}: {accuracy_score:.4f}\")\n",
    "\n",
    "        # Confusion matrix and metrics\n",
    "        predicted_classes = np.argmax(model.predict(test_feature_vector), axis=1)\n",
    "        cm = confusion_matrix(test_label, predicted_classes)\n",
    "        print(f\"Confusion Matrix for split {split_num + 1}:\\n{cm}\")\n",
    "\n",
    "        print(test_label, '____________ test_label')\n",
    "        print(predicted_classes, '____________ predicted_classes')\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "        # Calculate sensitivity and specificity\n",
    "        sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "        # Save metrics\n",
    "        save_performance_summary(epoch_number, sensitivity, specificity, accuracy_score)\n",
    "        accuracy_list.append(accuracy_score)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "\n",
    "        # Plot training & validation accuracy and loss\n",
    "        fig, ax1 = plt.subplots(2, 1, figsize=(10, 10))\n",
    "        ax1[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        ax1[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        ax1[0].set_title(f'Model Accuracy :: Epochs :: {epoch_number}     Split No: {split_num + 1}')\n",
    "        ax1[0].set_ylabel('Accuracy')\n",
    "        ax1[0].set_xlabel('Epoch')\n",
    "        ax1[0].legend(loc='upper left')\n",
    "\n",
    "        ax1[1].plot(history.history['loss'], label='Training Loss')\n",
    "        ax1[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "        ax1[1].set_title(f'Model Loss :: Epochs :: {epoch_number}')\n",
    "        ax1[1].set_ylabel('Loss')\n",
    "        ax1[1].set_xlabel('Epoch')\n",
    "        ax1[1].legend(loc='upper left')\n",
    "\n",
    "        annotation_text = (\n",
    "            f\"{test_category}\\n\"\n",
    "            f\"{'Accuracy '.ljust(18)}:  {accuracy_score:9.2f}\\n\"\n",
    "            f\"{'Sensitivity '.ljust(18)}: {sensitivity:10.2f}\\n\"\n",
    "            f\"{'Specificity'.ljust(18)}: {specificity:10.2f}\\n\"\n",
    "            \"\\n\"\n",
    "            f\"{'Avg. Accuracy'.ljust(18)}: {np.mean(accuracy_list):10.2f}\\n\"\n",
    "            f\"{'Avg. Sensitivity'.ljust(18)}: {np.mean(sensitivity_list):10.2f}\\n\"\n",
    "            f\"{'Avg. Specificity'.ljust(18)}: {np.mean(specificity_list):10.2f}\\n\"\n",
    "        )\n",
    "\n",
    "        ax1[0].text(x_distance, \n",
    "                    y_distance, \n",
    "                    annotation_text, transform=ax1[0].transAxes, \n",
    "                    fontsize = get_font_size, \n",
    "                    fontfamily= get_font, \n",
    "                    bbox=dict(facecolor='white', alpha=transparency_level))\n",
    "        \n",
    "        os.makedirs(folder_name, exist_ok=True)  # Ensure folder exists\n",
    "        plt.tight_layout()\n",
    "        current_time = datetime.now().strftime(\"%H_%M_%S\")\n",
    "        plt.savefig(os.path.join(folder_name, f'{folder_name}_{current_time}.png'))\n",
    "        plt.close()\n",
    "\n",
    "# Calculate and display average metrics\n",
    "average_accuracy = np.mean(accuracy_list)\n",
    "average_sensitivity = np.mean(sensitivity_list)\n",
    "average_specificity = np.mean(specificity_list)\n",
    "print(f\"\\nAverage Accuracy: {average_accuracy:.2f}\")\n",
    "print(f\"Average Sensitivity: {average_sensitivity:.2f}\")\n",
    "print(f\"Average Specificity: {average_specificity:.2f}\")\n",
    "\n",
    "# End time\n",
    "end_time = datetime.now()\n",
    "print(f\"Total Duration: {end_time - start_time}\")\n",
    "\n",
    "\n",
    "print()\n",
    "print()\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
